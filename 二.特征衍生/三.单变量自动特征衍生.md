# 一.单变量自动特征衍生

### 1.数据重编码特征衍生

  首先需要知道的是，此前我们所介绍的所有数据重编码的过程，新创建的列都可以作为一个额外的独立特征，即我们在实际建模过程中，不一定是用重编码后新的列替换掉原始列，而是考虑同时保留新的特征和旧的特征，带入到下一个环节、即特征筛选来进行特征筛选，如果数据重编码后的特征是有效的，则自然会被保留，否则则会被剔除。

  这么一来，我们或许就不用考虑在当前模型下是否需要进行数据重编码，而是无论是否需要，都先进行重编码、并同时保留原特征和衍生出来的新的特征。此处我们简单回顾此前所介绍的一系列数据重编码的方法：

- 连续变量数据重编码方法
  - 标准化：0-1标准化/Z-Score标准化
  - 离散化：等距分箱/等频分箱/聚类分箱

- 离散变量数据重编码方法
  - 自然数编码/字典编码
  - 独热编码/哑变量变换

  当然，在同时保留原始列和重编码的列时，极有可能出现原始列和重编码的列都是有效特征的情况，例如此前我们看到的tenure独热编码后的某列（tenure=1）和原始列同时带入模型的情况。

### 2.高阶多项式特征衍生

  对于单独的变量来说，除了可以通过重编码进行特征衍生外，还可以通过多项式进行特征衍生，即创建一些自身数据的二次方、三次方数据等。该方法我们曾在逻辑回归一节中重点介绍过，此处进行简单回顾。

- 多项式处理结果

  假设$X1$是某原始特征，单独特征进行高阶多项式衍生过程如下：

![image-20220119174222374](https://s2.loli.net/2022/01/19/t37jW9xZ5kqUeGd.png)

- 多项式衍生实现方法

  上述过程较为简单，直接利用数组广播特性，手动实现过程也较为简单。当然我们更推荐使用sklearn中的PolynomialFeatures评估器来实现该过程。该评估器不仅能够非常便捷的实现单变量的多项式衍生，也能够快速实现多变量组合多项式衍生，且能够与机器学习流集成，也便于后续的超参数搜索方法的使用。

```python
from sklearn.preprocessing import PolynomialFeatures
x1 = np.array([1, 2, 4, 1, 3])
PolynomialFeatures(degree=5).fit_transform(x1.reshape(-1, 1))
```

尽管过程简单，但多项式特征衍生也是极为常见且效果出色的特征衍生方法，在此前的逻辑回归建模实验中，简单的多项式衍生，就能够将逻辑回归的决策边界由线性改善至不规则边界，从而极大提升模型建模效果：

![image-20220119180207156](https://s2.loli.net/2022/01/19/mUEtDxkquaMWh1w.png)

当然，一般来说单特征的多项式往往是针对连续变量会更有效，但再某些情况下也可以将其用于离散型随机变量。

### 3.特征衍生准则

- 无限特征

  在上述过程的基础上，我们将这些方法稍作组合、或者是进行深入拓展，则会发现，就单独一个特征而言，我们都可以衍生出无限个特征。例如我们可以将某连续变量先进行N阶多项式衍生，然后再进行分箱，然后再进行独热编码；或者就是单纯的进行非常高阶的多项式衍生；再或者，我们看随机修改下归一化的规则（以0-1标准化为例），不再是减去最小值除以极值，而是减去次小的值、或者第三小的值等等。你会发现，哪怕是单独针对某个变量，我们都可以衍生出近乎无穷个特征。

  但需要知道的是，尽管特征可以无限衍生，但因为算力有限、时间有限，我们不可能进行无止尽的尝试。因此，在实际模型训练过程中，也并非无节制的朝向无限特征的方向进行特征衍生，往往我们需要有些判断，即哪些情况下朝什么方向进行特征衍生是最有效的。当然，同时我们需要知道的是，特征衍生的方法极少有理论依据、或者只有零散的理论依据，例如多项式衍生其实是借助了核函数的思想，但就特征衍生的技术整体而言，并没有一个完整系统的理论体系，作为何时选择何种方法的参考依据。因此不难看出，特征衍生其实是一个极其考验建模工作者的建模经验、数据敏感度甚至是建模灵感的工作事项，而在很多实际工作和竞赛中，我们也确实会发现特征工程方法选择的不同，往往是建模结果拉开差距的关键。

> 随着特征衍生进行的深入，新特征的有效性也是在快速递减的。

  而在茫茫多特征衍生方向中如何选择当前数据集特征衍生的方向，首先，课上会在每套特征衍生方法的后面附上这些方法的选择依据，这些依据大都源自实践经验，可以普遍适用于一般情况，并作为特征衍生的基本依据；其次，在后续的案例课中，我们也将结合具体的数据来讨论不同情况下最适用的特征衍生方法，在实践中快速积累特征衍生方法的使用经验。

- **特征衍生选择依据**

  **这里我们先给出上述单变量特征衍生方法在使用过程中的选取依据，此处我们假设实际构建的模型以集成学习为主：**

- **优先考虑分类变量的独热编码，并同时保留原始变量与独热编码衍生后的变量。独热编码能够丰富树模型生长过程中备选的数据集切分点，因此能够进一步丰富集成学习中不同树模型可能的差异性。但同时也需要注意的是有两种情况不适用于使用独热编码，其一是分类变量取值水平较多（例如超过10个取值），此时独热编码会造成特征矩阵过于稀疏，从而影响最终建模效果；其二则是如果该离散变量参与后续多变量的交叉衍生（稍后会介绍），则一般需再对单独单个变量进行独热编码；**
- **优先考虑连续变量的数据归一化，尽管归一化不会改变数据集分布，即无法通过形式上的变换增加树生长的多样性，但归一化能够加快梯度下降的执行速度，加快迭代收敛的过程；**
- **在连续变量较多的情况下，可以考虑对连续变量进行分箱，原因同第一点。具体分享方法优先考虑聚类分箱，若数据量过大，可以使用MiniBatch K-Means提高效率，或者也可以简化为等频率/等宽分箱；**
- **不建议对单变量使用多项式衍生方法，相比单变量的多项式衍生，带有交叉项的多变量的多项式衍生往往效果会更好。**