# 一.数据集探查检验

## 1.数据字段释义

### 1.1 查看的步骤：

1. 查看数据集有多少个字段
2. 查看数据集有多少条
3. 列出每个字段的含义
4. 查看每个字段的数据类型
5. 列出每个字段是离散型变量还是连续型变量，举例：

```python
# 离散字段
category_cols = [ 'gender', 'SeniorCitizen', 'Partner', 'Dependents',
                'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 
                'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling',
                'PaymentMethod']

# 连续字段
numeric_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']

# 标签
target = 'Churn'

#ID 列
ID_col = 'customerID'

# 验证是否划分能完全
assert len(category_cols) + len(numeric_cols) + 2 == tcc.shape[1]
```



### 1.2 常用的代码：

```python
df.info()
```

## 2.数据集正确性校验

### 2.1 检查某列是否有重复值(比如ID列)：

```python
df['ID'].nunique() == df.shape[0]
```

### 2.2 检查是否有重复的两行

```python
df.duplicated().sum()
```

## 3.缺失值检查

### 3.1 使用is_null()函数检查

```python
df.isnull().sum()
```

### 3.2 自定义函数

自定义函数打印每列的缺失值和缺失值比例

```python
def missing(df):
    """
        计算每列的缺失值占比
    """
    missing_number = df.isnull().sum().sort_values(ascending=False)
    missing_percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)
    missing_values = pd.concat([missing_number, missing_percent], axis=1, keys=['Missing_Number', 'Missing_Percent']) 
    return missing_values
```

### 3.3 检查其他缺失值可能的情况

**方法一：**

is_null()函数的有局限性，该函数只能检测出None或者nan的情况，但有时候缺失值可能空格或者其他字符等。

有一种办法就是尝试进行类型转换：

```python
df[numeric_cols].astype(float)
```

如果报错，则可能存在某种字符在代表空值。以空格为例，列出检查方法：

```python
def find_index(data_col, val):
    """
        查询某值在某列中第一次出现位置的索引，没有则返回-1
        :param data_col:查询的列
        :param val:具体取值
    """
    val_list = [val]
    if data_col.isin(val_list).sum() == 0:
        index = -1
    else:
        index = data_col.isin(val_list).idxmax()
    return index   
```

```python
for col in numeric_cols:
    print(col, ":", find_index(df[col], " "))
```

将空格代表的字符串，转化为np.nan:

```python
df['TotalCharges'] = df['TotalCharges'].apply(lambda x:x if x!=' ' else np.nan)
```

**方法二：**

还有另一种更便捷的方法，使用pd.numeric()函数对连续变量进行转化，设置errors='coerce'，表示对于可以转换数值类型的直接转换，无法转换的用np.nan进行填充：

```python
pd.to_numeric(df.TotalCharges, errors='coerce') 
df.TotalCharges.isnull().sum()
#to_numeric函数可以自动处理无法转化的数值的数值都填补成nan
```

## 4.异常值检验

### 4.1 3σ法

3σ法是指通过三倍标准差法来检验，即以均值- 3σ 为下界，均值+3σ为上界，来检查是否有超过边界的点。

```python
df['MonthlyCharges'].mean() + 3 * df['MonthlyCharges'].std()
```

```python
df['MonthlyCharges'].mean() - 3 * df['MonthlyCharges'].std()
```

### 4.2 箱线图法

箱线图法主要是借助中位数和四分位数来进行计算，以上四分位数+1.5倍四分位距为上界，下四分位数-1.5倍的四分位距为下界，超出界限的部分认为是异常值。

**手动计算：**

```python
#求上四分位数
Q3 = df[numeric_cols].describe()['MonthlyCharges']['75%']
#求下四分位数
Q1 = df[numeric_cols].describe()['MonthlyCharges']['25%']
#求四分位距
IQR = Q3 - Q1
#求异常值的上界
Q3 + 1.5 * IQR
#求异常值的下界
Q1 - 1.5 * IQR
#求最大值最小值然后和上下界进行比较
df['MonthlyCharges'].min(), df['MonthlyCharges'].max()
```

```python
#封装一个函数
def check_outlier_by_IQR(df, cols):
    result = []
    for col in cols:
        Q3 = df.describe()[col]['75%']
        Q1 = df.describe()[col]['25%']
        IQR = Q3 - Q1
        up = Q3 + 1.5 * IQR
        down = Q1 - 1.5 * IQR
        maximum = df[col].max()
        minimum = df[col].min()
        up_exceed = up < maximum
        down_exceed = down > minimum
        result.append({'name':col, 'Q1':Q1, 'Q3':Q3, 'IQR':IQR, 'up':up, 'down':down, 'max':maximum, 'min':minimum, 'up_exceed':up_exceed, 'down_exceed':down_exceed})
    return pd.DataFrame(result)  
```

除去手动计算，绘图效果更为直观：

**绘图：**

```python
plt.figure(figsize=(12, 6))
plt.subplot(121)
plt.boxplot(df['MonthlyCharges'])
plt.xlabel('MonthlyCharges')
```

**注意：**

matplotlib里面的箱线图和标准的箱线图区别？

matplotlib如果没有异常点，那么上线的边界会取最大值和最小值，而不是+-1.5倍四分位距的数值。

### 4.3如何在3σ和箱线图法中抉择？

如果数据的偏态比较严重，那么更应该相信箱线图法。如果基本服从正态分布，那么都可以用。

可以使用histplot()查看正太分布情况。

```python
plt.figure(figsize=(12, 6))
plt.subplot(121)
sns.histplot(df['MonthlyCharges'], kde=True)
```

## 5.变量相关性探索分析

### 5.1 标签的取值分布

```python
y = df['Churn']
print(f'Percentage of churn :{round(y.value_counts(normalize=True)[0] * 100, 2)} % --> ({y.value_counts()[0]} customer)')
print(f'Percentage of customer dit not churn :{round(y.value_counts(normalize=True)[1] * 100, 2)} % --> ({y.value_counts()[1]} customer)')
```

```python
sns.displot(y)
```

### 5.2 变量相关性分析

从严格的统计学意义来讲，不同类型变量的相关性需要采用不同的分析方法：  
1.连续变量之间相关性可以使用皮尔逊相关系数进行计算。  
2.连续变量和离散变量之间相关性则可以卡方检验进行分析。  
3.离散变量之间则可以使用信息增益角度入手分析。  
但如果只是想初步探查变量之间的相关关系，则可以忽略变量的离散和连续特性，使用相关系数来进行计算。 

**相关系数矩阵：**

为了更好的分析分类变量如何影响标签的取值，我们需要将标签转化为整型。（也就是视作连续变量），而将所有的分类变量进行哑变量处理。 

```python
# 剔除ID列
df3 = df.iloc[:, 1:].copy()
# 将标签Yes/No 转化为1/0
df3['Churn'].replace(to_replace='Yes', value=1, inplace=True)
df3['Churn'].replace(to_replace='No', value=0, inplace=True)
# 将其他所有分类变量转化为哑变量， 连续变量保持不变
df_dummies = pd.get_dummies(df3)
```

```
df_dummies.corr()
```

在所有相关性中，应该重点观察特征和标签之间的相关关系，因此可以直接挑选标签列的相关系数计算结果，进行降序排列：

```python
df_dummies.corr()['Churn'].sort_values(ascending = False)
```

需要明确的是，相关系数的计算基本原理，相关系数如果是正数，则二者为正相关，数值变化会更倾向于保持同步。相关系数为负数，则负相关。

**热力图展示：**

```python
plt.figure(figsize=(12, 6))
sns.heatmap(df_dummies.corr())
```

热力图如果特征较多，展示结果则不够直观，可以只对标签的相关系数进行柱状图的展示：

```python
sns.set()
plt.figure(figsize=(12,6))
df_dummies.corr()['Churn'].sort_values(ascending=False).plot(kind='bar')
```

### 5.3 探查列的取值分布和标签直接的关系

**封装函数：**

```python
def check_col_target_relation(df, cols, target, nrows, ncols, dodge):
    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(16, 12), dpi=100)
    for i, item in enumerate(cols):
        plt.subplot(nrows, ncols, (i+1))
        ax = sns.countplot(x=item, hue="Churn", data=df, palette='Blues', dodge=dodge)
        plt.xlabel(item)
        plt.title(f'{target} by ' + item)
```

```python
cols = ['gender', 'SeniorCitizen', 'Partner', 'Dependents']
check_col_target_relation(df, cols, "Churn", 2, 2, dodge=False)
```

